# -*- coding: utf-8 -*-
"""M515Y1092_Erin_Sistem_Rekomendasi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DPqi95s0TTl1y1wEMzVgU5Etywbq1Ooz

Nama : Erin Nur Fatimah

ID : M515Y1092

Alamat : Sindet RT 003, Wukirsari, Imogiri, Bantul, Yogyakarta

# Data Loading

### Mengimport drive ke google colab
"""

from google.colab import drive
drive.mount('/content/drive')

"""### Mengimport Library yang Dibutuhkan"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
# %matplotlib inline
from pathlib import Path
import seaborn as sns
from sklearn.model_selection import train_test_split

"""# **Data Understanding**

## Memuat Data pada sebuah Dataframe menggunakan pandas
"""

books = pd.read_csv('/content/drive/MyDrive/data set/data set proyek 2/book_recommender/Books.csv')
ratings = pd.read_csv('/content/drive/MyDrive/data set/data set proyek 2/book_recommender/Ratings.csv')

"""Mengambil data buku dan rating yang digunakan masing-masing sebanyak 14000"""

dt_books = books.iloc[:14000]
dt_ratings = ratings.iloc[:14000]

len(dt_books)
len(dt_ratings)

"""Mengubah "-" menjadi "_" agar mempermudah proses pemanggilan"""

dt_books.columns = books.columns.str.lower()
dt_ratings.columns = ratings.columns.str.lower()

dt_books.columns = books.columns.str.replace("-","_")
dt_ratings.columns = ratings.columns.str.replace("-","_")

"""# **Univariate**

## Book Variabel

### Membuat kolom type pada dataset
"""

dt_books.info()

print('Jumlah Buku berdasar Rating: ', len(dt_ratings.ISBN.unique()))
print('Jumlah Buku berdasar Daftar Buku: ', len(dt_books.ISBN.unique()))

"""Menghapus 3 kolom pada 'data_books' karena tidak akan digunakan"""

dt_books.drop(['Image_URL_S', 'Image_URL_M', 'Image_URL_L' ], axis=1, inplace=True)

"""Mengubah kolom 'Year_Of_Publication' yang semula bertipe object menjadi integer"""

dt_books['Year_Of_Publication'] = pd.to_numeric(dt_books['Year_Of_Publication'], errors='coerce')

dt_books.info()

"""## Ratings Variabel

### Melihat data pada variabel rating dengan fungsi head()
"""

dt_ratings.head()

"""### Melihat Distribusi Rating"""

dt_ratings.describe()

"""### Melihat berapa pengguna yang memberikan rating, jumlah ISBN, dan jumlah rating"""

print('Jumlah user_ID: ', len(dt_ratings.User_ID.unique()))
print('Jumlah ISBN: ', len(dt_ratings.ISBN.unique()))
print('Jumlah data rating: ', len(dt_ratings))

"""# Data Preprocesing

## Menggabungkan dt_books dan dt_ratings
"""

dt = dt_ratings.merge(dt_books, left_on = 'ISBN', right_on = 'ISBN')
dt

dt.info()

most_author = dt.Book_Author.value_counts().reset_index()
most_author.columns = ['Book_Author','count']

plt.figure(figsize = (16,8))
plt.title("Top 10 Autors")
sns.barplot(x = 'count', y = 'Book_Author', data = most_author.head(10), palette='icefire_r');
plt.ylabel('Book_Author')
plt.xlabel('Count')
plt.show()

"""# **Data Preparation**

## Mengatasi Missing Value

Menghitung jumlah data kosong pada setiap kolom
"""

dt_books.isnull().sum()

dt_ratings.isnull().sum()

dt.isnull().sum()

"""## Membuat variabel bernama preparation"""

preparation = dt
preparation.sort_values('ISBN')

"""## Membuang data duplikat pada variabel preparation dengan fungsi drop_duplicates()"""

preparation = preparation.drop_duplicates('ISBN')
preparation

"""## Melakukan konversi data series menjadi list"""

books_id = preparation['ISBN'].tolist()
books_title = preparation['Book_Title'].tolist()
books_author = preparation['Book_Author'].tolist()
 
print(len(books_id))
print(len(books_title))
print(len(books_author))

"""## Membuat dictionary untuk menentukan pasangan key-value pada data books_id, books_title, dan books_author"""

books_new = pd.DataFrame({
    'id': books_id,
    'title':books_title,
    'author': books_author
})
books_new

"""# **Model Development dengan Content Based Filtering**

## Assign dataframe dari tahap sebelumnya ke dalam variabel data
"""

data = books_new
data.sample(5)

"""## TF-IDF Vectorizer

### Menggunakan fungsi tfidfvectorizer() dari library sklearn
"""

from sklearn.feature_extraction.text import TfidfVectorizer
 
# Inisialisasi TfidfVectorizer
tf = TfidfVectorizer()
 
# Melakukan perhitungan idf pada data title
tf.fit(books_new['title']) 
 
# Mapping array dari fitur index integer ke fitur nama
tf.get_feature_names()

"""### Melakukan fit dan transformasi ke dalam bentuk matriks"""

tfidf_matrix = tf.fit_transform(books_new['title']) 
tfidf_matrix.shape

"""### Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()"""

tfidf_matrix.todense()

"""### Matriks tf-idf untuk beberapa Books (title) dan author"""

pd.DataFrame(
    tfidf_matrix.todense(), 
    columns=tf.get_feature_names(),
    index=data.author
).sample(22, axis=1).sample(10, axis=0)

"""## Cosine Similarity

### Menggunakan fungsi cosine_similarity dari library sklearn
"""

from sklearn.metrics.pairwise import cosine_similarity

cosine_sim = cosine_similarity(tfidf_matrix) 
cosine_sim

"""### Membuat dataframe dari variabel cosine_sim_df dengan baris dan kolom berupa author"""

cosine_sim_df = pd.DataFrame(cosine_sim, index=books_new['author'], columns=books_new['author'])
print('Shape:', cosine_sim_df.shape)
 
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""## Mendapatkan Rekomendasi"""

def books_recommendations(Books_Author, similarity_data=cosine_sim_df, items=books_new[['title', 'author']], k=5):
   
    index = similarity_data.loc[:,Books_Author].to_numpy().argpartition(
        range(-1, -k, -1))
    
    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    closest = closest.drop(Books_Author, errors='ignore')
 
    return pd.DataFrame(closest).merge(items).head(k)

books_new[books_new.author.eq('Robert Hendrickson')]

"""### Mendapatkan rekomendasi Buku yang mirip dengan Robert Hendrickson"""

books_recommendations('Robert Hendrickson')

"""# **Model Development dengan Collaborative Filtering**

## Membaca dataset
"""

data = dt_ratings
data

"""## Menyandikan (encode) fitur User_ID dan ISBN ke dalam indeks integer"""

# Mengubah UserID menjadi list tanpa nilai yang sama
user_ids = data['User_ID'].unique().tolist()
print('list User_ID: ', user_ids)
 
# Melakukan encoding userID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded User_ID : ', user_to_user_encoded)
 
# Melakukan proses encoding angka ke UserID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke User_ID: ', user_encoded_to_user)

"""## Melakukan Proses Encoding Pada ISBN"""

book_ids = data['ISBN'].unique().tolist()
book_to_book_encoded = {x: i for i, x in enumerate(book_ids)}
book_encoded_to_book = {i: x for i, x in enumerate(book_ids)}

"""## Petakan User_ID dan ISBN ke dataframe yang berkaitan"""

# Mapping userID ke dataframe user
data['user'] = data['User_ID'].map(user_to_user_encoded)
 
# Mapping ISBN ke dataframe book
data['book'] = data['ISBN'].map(book_to_book_encoded)

"""## Cek beberapa hal dalam data seperti jumlah user, jumlah ISBN, dan mengubah nilai ratings menjadi float"""

num_users = len(user_to_user_encoded)
print(num_users)
 
num_book = len(book_encoded_to_book)
print(num_book)
 
# Mengubah rating menjadi nilai float
data['Book_Rating'] = data['Book_Rating'].values.astype(np.float32)
 
min_rating = min(data['Book_Rating'])
max_rating = max(data['Book_Rating'])
 
print('Number of User: {}, Number of ISBN: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_book, min_rating, max_rating
))

"""## Membagi Data untuk Training dan Validasi

### Mengacak dataset
"""

data = data.sample(frac=1, random_state=42)
data

"""### Membagi data train dan validasi dengan komposisi 80:20"""

x = data[['user', 'book']].values
y = data['Book_Rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values
 
# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * data.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)
 
print(x, y)

"""# Proses Training"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
class RecommenderNet(tf.keras.Model):
 
  # Insialisasi fungsi
  def __init__(self, num_users, num_book, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_book = num_book
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( 
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) 
    self.book_embedding = layers.Embedding( 
        num_book,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.book_bias = layers.Embedding(num_book, 1) 
 
  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) 
    user_bias = self.user_bias(inputs[:, 0]) 
    book_vector = self.book_embedding(inputs[:, 1]) 
    book_bias = self.book_bias(inputs[:, 1]) 
 
    dot_user_book = tf.tensordot(user_vector, book_vector, 2) 
 
    x = dot_user_book + user_bias + book_bias
    
    return tf.nn.sigmoid(x)

"""# Evaluation

## Melakukan proses compile terhadap model
"""

model = RecommenderNet(num_users, num_book, 50) 
 
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""## Memulai proses training dengan batch size sebesar 8 serta epoch 25 kali"""

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 25,
    validation_data = (x_val, y_val)
)

"""## Visualisasi Metrik"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""## Mendapatkan Rekomendasi Buku"""

book_df = books_new
 
User_ID = data.User_ID.sample(1).iloc[0]
book_visited_by_user = data[data.User_ID == User_ID]
 
book_not_visited = book_df[~book_df['id'].isin(book_visited_by_user.ISBN.values)]['id'] 
book_not_visited = list(
    set(book_not_visited)
    .intersection(set(book_to_book_encoded.keys()))
)
 
book_not_visited = [[book_to_book_encoded.get(x)] for x in book_not_visited]
user_encoder = user_to_user_encoded.get(User_ID)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_visited), book_not_visited)
)

"""## Memperoleh rekomendasi buku menggunakan fungsi model.predict() dari library Keras"""

ratings = model.predict(user_book_array).flatten()
 
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_book_ids = [
    book_encoded_to_book.get(book_not_visited[x][0]) for x in top_ratings_indices
]
 
print('Showing recommendations for users: {}'.format(User_ID))
print('===' * 12)
 
top_book_user = (
    book_visited_by_user.sort_values(
        by = 'Book_Rating',
        ascending=False
    )
    .head(5)
    .ISBN.values
)
 
book_df_rows = book_df[book_df['id'].isin(top_book_user)]
for row in book_df_rows.itertuples():
    print(row.title, ':', row.author)
 
print('----' * 12)
print('Top 10 books recommendation')
print('----' * 12)
 
recommended_book = book_df[book_df['id'].isin(recommended_book_ids)]
for row in recommended_book.itertuples():
    print(row.title, ':', row.author)